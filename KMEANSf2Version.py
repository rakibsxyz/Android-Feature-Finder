# -*- coding: utf-8 -*-
"""kmeansonVDO+CALCUwithWordFindingF1version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PGaXNoZ0vJwBnIqHExqkHM0ObWYZvicH
"""

def runKmeansAndCluster(sourceFileForPrediction, clusters):
  import numpy as np
  import pandas as pd
  from matplotlib import pyplot as plt
  from sklearn.cluster import KMeans
  #import sklearn.cluster.hierarchical as hclust
  from sklearn import preprocessing
  import seaborn as sns
  from sklearn.feature_extraction.text import TfidfVectorizer

  df = pd.read_csv(sourceFileForPrediction)
  copyDf = df

  print(df.shape)
  df.head()

  v = TfidfVectorizer()
  # x = v.fit_transform(df['activity'].values.astype('U'))
  # # x = v.fit_transform(df["activity"])

  # df["activityTf"] = x

  # df["activityTf"] = x.toarray()

  # x = v.fit_transform(df['fragment'].values.astype('U'))
  # df["fragmentTf"] = x
  # df["fragmentTf"] = x.toarray()

  # x = v.fit_transform(df["class Names"])
  v = TfidfVectorizer()
  x = v.fit_transform(df['methodStact'].values.astype('U'))
  df["methodTf"] = x
  df["methodTf"] = x.toarray()

  # x = v.fit_transform(df["class Name Token"])
  x = v.fit_transform(df['packagename'].values.astype('U'))
  df["pkgTf"] = x
  df["pkgTf"] = x.toarray()

  x = v.fit_transform(df['methodName'].values.astype('U'))
  df["methodNameTf"] = x
  df["methodNameTf"] = x.toarray()

  x = v.fit_transform(df['className'].values.astype('U'))
  df["classNameTf"] = x
  df["classNameTf"] = x.toarray()

  df.columns

  df = df.dropna(axis=0, subset=['MSLength'])

  features = df.drop(['activity', 'fragment', 'methodStact' , 'packagename', 'methodName' , 'className', 'allPackagMethodName' ],axis=1)

  features.describe()

  scaler = preprocessing.MinMaxScaler()
  features_normal = scaler.fit_transform(features)

  pd.DataFrame(features_normal).describe()

  inertia = []

  K = range(1,clusters)
  for k in K:
      kmeanModel = KMeans(n_clusters=k).fit(features_normal)
      kmeanModel.fit(features_normal)
      inertia.append(kmeanModel.inertia_)

  # plt.plot(K, inertia, 'bx-')
  # plt.xlabel('k')
  # plt.ylabel('Inertia')
  # plt.show()

  totalCluster = clusters
  print(totalCluster)

  kmeans = KMeans(n_clusters=totalCluster).fit(features_normal)

  labels = pd.DataFrame(kmeans.labels_) #This is where the label output of the KMeans we just ran lives. Make it a dataframe so we can concatenate back to the original data
  labeledMethodStack = pd.concat((features,labels),axis=1)
  labeledMethodStack = labeledMethodStack.rename({0:'labels'},axis=1)

  labeledMethodStack.head()

  # sns.lmplot(x='classNameTf',y='MSLength',data=labeledMethodStack,hue='labels',fit_reg=False)

  # sns.pairplot(labeledMethodStack,hue='labels')

  labeledMethodStack['Constant'] = "Data"

  # sns.stripplot(x=labeledMethodStack['Constant'],y=labeledMethodStack['MSLength'],hue=labeledMethodStack['labels'],jitter=True)

  # sns.swarmplot(x=labeledMethodStack['Constant'],y=labeledMethodStack['method length'],hue=labeledMethodStack['labels'])

  # copyDf.head

  # methodStacks = df['Method Stack', 'Method Names', 'class Names' , 'class Name Token']
  methodStacks = pd.concat((copyDf,labels),axis=1)
  methodStacks = methodStacks.rename({0:'Cluster'},axis=1)
  sortMethodStack = methodStacks.sort_values(['Cluster'])
  pd.set_option('display.max_rows', 1000)
  # sortMethodStack

  # print(len(sortMethodStack['Cluster'==0]))
  # sortMethodStack.to_csv("/content/drive/MyDrive/sPL3/ktraceSortedClusterAllAtrrib.csv")

  # print(methodStacks['Clsuter'] == 1).sum()
  clus = sortMethodStack['Cluster']
  # print(clus)

  x = 0
  for i in range(0,totalCluster):
    print(list(methodStacks.Cluster).count(i))
    x+= list(methodStacks.Cluster).count(i)
    # methodStacks.query('Cluster == i').Cluster.count()
  print(x)

  import pandas as pd
  df = sortMethodStack

  # clusterNumber = 3
  activityNameList = []
  fragmentNameList = []
  methodNameList = []
  packageMethodNameList =[]
  for i in  range(0,totalCluster):
    activityNameList.append(df.loc[df["Cluster"] == i, "activity"])
    fragmentNameList.append(df.loc[df["Cluster"] == i, "fragment"])
    methodNameList.append(df.loc[df["Cluster"] == i, "methodName"])
    packageMethodNameList.append(df.loc[df["Cluster"] == i, "allPackagMethodName"])

  # from nltk.tokenize import word_tokenize
  # import nltk 
  # nltk.download('punkt')

  check=0

  activityTokenClusters = []
  for i in range(len(activityNameList)):
    activityTokens =[]
    for item in activityNameList[i]:
      activityTokens.append(item)
      # activityTokens.append(item.split(" "))
        # print(methodTokens)
    activityTokenClusters.append(activityTokens)
  print(len(activityNameList[0]))

  len(activityTokenClusters)

  fragmentTokenClusters = []
  for i in range(len(fragmentNameList)):
    fragmentTokens =[]
    for item in fragmentNameList[i]:
      fragmentTokens.append(item)
      # activityTokens.append(item.split(" "))
        # print(methodTokens)
    fragmentTokenClusters.append(fragmentTokens)
  # print(fragmentNameList[0])

  from sklearn.feature_extraction.text import TfidfVectorizer
  # v = TfidfVectorizer()
  v = TfidfVectorizer(max_features=10)
  # tfidf = v.fit_transform(df['Method Names'])
  # tfidf = v.fit_transform(activityNameList[0])
  # tfidf= v.fit_transform(activityNameList[0].values.astype('U'))

  # v2 = TfidfVectorizer(max_features=10)
  # tfidf2= v2.fit_transform(fragmentNameList[0].values.astype('U'))

  # import numpy as np
  # v.get_feature_names()

  # names = v2.get_feature_names()
  # print(names)

  acvityFeatureName = []
  fragmentFeatureName = []
  for i in range(0, len(activityNameList)):  
    v = TfidfVectorizer(max_features=10)
    v2 = TfidfVectorizer(max_features=10) 
    ai = False
    fi = False
    try:
      fragmentTfidf = v.fit_transform(fragmentNameList[i].values.astype('U'))
    except ValueError:
      fragmentFeatureName.append("null")
      fi = True
      # print("fak")
    try:
      activityTfidf= v2.fit_transform(activityNameList[i].values.astype('U'))
    except ValueError:
      acvityFeatureName.append("null")
      ai = True
    # activityTfidf= v2.fit_transform(activityNameList[i].split(' '))
    # fragmentTfidf = v.fit_transform(fragmentNameList[i].split(' '))
    if(ai== True and fi== True):
      # print("all True")
      continue
    if(ai == False):
      acvityFeatureName.append(v2.get_feature_names())
      # print("yesh")
    if(fi == False):
      fragmentFeatureName.append(v.get_feature_names())
      # print("fak2")

  for i in range(0,len(acvityFeatureName)):
    print(acvityFeatureName[i])
  len(acvityFeatureName)

  for i in range(0,len(fragmentFeatureName)):
    print(fragmentFeatureName[i])
  len(fragmentFeatureName)
  newFragmentFeatureName = fragmentFeatureName
  for i in range(0,len(newFragmentFeatureName)):
    if(str(fragmentFeatureName[i]) =="null"):
    # print(len(newFragmentFeatureName[i])," len bro")
    # if(len(newFragmentFeatureName[i])<2):
      # print("oknull")
      v2 = TfidfVectorizer(max_features=10)
      tfidf2= v2.fit_transform(packageMethodNameList[i].values.astype('U'))
      newFragmentFeatureName[i] = v2.get_feature_names()
      # print(fragmentFeatureName[i])

  def word2vec(word):
      from collections import Counter
      from math import sqrt

      # count the characters in word
      cw = Counter(word)
      # precomputes a set of the different characters
      sw = set(cw)
      # precomputes the "length" of the word vector
      lw = sqrt(sum(c*c for c in cw.values()))

      # return a tuple
      return cw, sw, lw

  def cosdis(v1, v2):
      # which characters are common to the two words?
      common = v1[1].intersection(v2[1])
      # by definition of cosine distance we have
      return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]

  listOfWordsFeature =[]
  listFeature =[]
  threshold = 0.5  # if needed

   

  for k in range(0,totalCluster):  
    list_A = newFragmentFeatureName[k]
    list_B = acvityFeatureName[k]
    wordSet = set()
    for key in list_A:
        for word in list_B:
            try:
                # print(key)
                # print(word)
                res = cosdis(word2vec(word), word2vec(key))
                # print(res)
                # print("The cosine similarity between : {} and : {} is: {}".format(word, key, res*100))
                if res > threshold:
                    print("Found a word with cosine distance > 80 : {} with original word: {}".format(word, key))
                    wordSet.add(word)
                    wordSet.add(key)
            except IndexError:
                pass
        # print(wordSet)

    listOfWordsFeature.append(wordSet)
    listFeature.append(k)

  
  return listOfWordsFeature,listFeature

# Tried for package MethodName

# Comment

# This one is improved for vdo files here
# Improvements [ Null values inserted ] [Fragments and activites are inserted nicely for vdo files]
#  To do : If null then what?