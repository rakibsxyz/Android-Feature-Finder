# -*- coding: utf-8 -*-
"""kmeansOnK9DatawithWordFinding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QsVcs8WImcHkfSmjSE44S02gdAQOpwTA
"""


def runKmeansAndCluster(sourceFileForPrediction):

  
  import numpy as np
  import pandas as pd
  from matplotlib import pyplot as plt
  from sklearn.cluster import KMeans
  #import sklearn.cluster.hierarchical as hclust
  from sklearn import preprocessing
  import seaborn as sns
  from sklearn.feature_extraction.text import TfidfVectorizer





  df = pd.read_csv(sourceFileForPrediction)
  # df = pd.read_csv('/content/drive/MyDrive/sPL3/calculator.csv')
  copyDf = df

  print(df.shape)
  df.head()

  v = TfidfVectorizer()
  # x = v.fit_transform(df['activity'].values.astype('U'))
  # # x = v.fit_transform(df["activity"])

  # df["activityTf"] = x

  # df["activityTf"] = x.toarray()

  # x = v.fit_transform(df['fragment'].values.astype('U'))
  # df["fragmentTf"] = x
  # df["fragmentTf"] = x.toarray()

  # x = v.fit_transform(df["class Names"])
  v = TfidfVectorizer()
  x = v.fit_transform(df['methodStact'].values.astype('U'))
  df["methodTf"] = x
  df["methodTf"] = x.toarray()

  # x = v.fit_transform(df["class Name Token"])
  x = v.fit_transform(df['packagename'].values.astype('U'))
  df["pkgTf"] = x
  df["pkgTf"] = x.toarray()

  x = v.fit_transform(df['methodName'].values.astype('U'))
  df["methodNameTf"] = x
  df["methodNameTf"] = x.toarray()

  x = v.fit_transform(df['className'].values.astype('U'))
  df["classNameTf"] = x
  df["classNameTf"] = x.toarray()

  df.columns

  df = df.dropna(axis=0, subset=['MSLength'])

  features = df.drop(['activity', 'fragment', 'methodStact' , 'packagename', 'methodName' , 'className', 'allPackagMethodName' ],axis=1)

  features.describe()

  scaler = preprocessing.MinMaxScaler()
  features_normal = scaler.fit_transform(features)

  pd.DataFrame(features_normal).describe()

# Elbow plot kortesina apatoto
  # inertia = []
  # print("ok on elbow")
  # K = range(1,11)
  # for k in K:
  #     kmeanModel = KMeans(n_clusters=k).fit(features_normal)
  #     kmeanModel.fit(features_normal)
  #     inertia.append(kmeanModel.inertia_)

  # plt.plot(K, inertia, 'bx-')
  # plt.xlabel('k')
  # plt.ylabel('Inertia')
  # plt.show()
 
  print("ok after  elbow")
  kmeans = KMeans(n_clusters=3).fit(features_normal)

  labels = pd.DataFrame(kmeans.labels_) #This is where the label output of the KMeans we just ran lives. Make it a dataframe so we can concatenate back to the original data
  labeledMethodStack = pd.concat((features,labels),axis=1)
  labeledMethodStack = labeledMethodStack.rename({0:'labels'},axis=1)

  labeledMethodStack.head()

  sns.lmplot(x='classNameTf',y='MSLength',data=labeledMethodStack,hue='labels',fit_reg=False)

  # sns.pairplot(labeledMethodStack,hue='labels')

  labeledMethodStack['Constant'] = "Data"

  sns.stripplot(x=labeledMethodStack['Constant'],y=labeledMethodStack['MSLength'],hue=labeledMethodStack['labels'],jitter=True)

  # sns.swarmplot(x=labeledMethodStack['Constant'],y=labeledMethodStack['method length'],hue=labeledMethodStack['labels'])

  copyDf.head
  print("ok after  ens plot")
  # methodStacks = df['Method Stack', 'Method Names', 'class Names' , 'class Name Token']
  methodStacks = pd.concat((copyDf,labels),axis=1)
  methodStacks = methodStacks.rename({0:'Cluster'},axis=1)
  sortMethodStack = methodStacks.sort_values(['Cluster'])
  pd.set_option('display.max_rows', 1000)
  # sortMethodStack

  # print(len(sortMethodStack['Cluster'==0]))
  # sortMethodStack.to_csv("/content/drive/MyDrive/sPL3/ktraceSortedClusterAllAtrrib.csv")

  # print(methodStacks['Clsuter'] == 1).sum()
  clus = sortMethodStack['Cluster']
  # print(clus)

  x = 0
  for i in range(0,3):
    print(list(methodStacks.Cluster).count(i))
    x+= list(methodStacks.Cluster).count(i)
    # methodStacks.query('Cluster == i').Cluster.count()
  # print(x)

  import pandas as pd
  df = sortMethodStack

  clusterNumber = 3
  activityNameList = []
  fragmentNameList = []
  for i in  range(0,clusterNumber):
    activityNameList.append(df.loc[df["Cluster"] == i, "activity"])
    fragmentNameList.append(df.loc[df["Cluster"] == i, "fragment"])

  from nltk.tokenize import word_tokenize
  import nltk 
  nltk.download('punkt')

  check=0

  activityTokenClusters = []
  for i in range(len(activityNameList)):
    activityTokens =[]
    for item in activityNameList[i]:
      activityTokens.append(item)
      # activityTokens.append(item.split(" "))
        # print(methodTokens)
    activityTokenClusters.append(activityTokens)
  # print(len(activityNameList[0]))

  len(activityTokenClusters)

  fragmentTokenClusters = []
  for i in range(len(fragmentNameList)):
    fragmentTokens =[]
    for item in fragmentNameList[i]:
      fragmentTokens.append(item)
      # activityTokens.append(item.split(" "))
        # print(methodTokens)
    fragmentTokenClusters.append(fragmentTokens)
  # print(fragmentNameList[0])

  from sklearn.feature_extraction.text import TfidfVectorizer
  # v = TfidfVectorizer()
  v = TfidfVectorizer(max_features=10)
  # tfidf = v.fit_transform(df['Method Names'])
  # tfidf = v.fit_transform(activityNameList[0])
  tfidf= v.fit_transform(activityNameList[0].values.astype('U'))

  v2 = TfidfVectorizer(max_features=10)
  tfidf2= v2.fit_transform(fragmentNameList[0].values.astype('U'))

  # import numpy as np
  v.get_feature_names()

  names = v2.get_feature_names()
  print(names)

  acvityFeatureName = []
  fragmentFeatureName = []
  for i in range(0, len(activityNameList)):  
    v = TfidfVectorizer(max_features=10)
    v2 = TfidfVectorizer(max_features=10) 
    fragmentTfidf = v.fit_transform(fragmentNameList[i].values.astype('U'))
    activityTfidf= v2.fit_transform(activityNameList[i].values.astype('U'))
    acvityFeatureName.append(v2.get_feature_names())
    fragmentFeatureName.append(v.get_feature_names())

  for i in range(0,len(acvityFeatureName)):
    print(acvityFeatureName[i])

  for i in range(0,len(fragmentFeatureName)):
    print(fragmentFeatureName[i])

  return True

